% Encoding: UTF-8

@Thesis{Brysbaert2021,
  author      = {Brysbaert, Sam},
  date        = {2021},
  institution = {HoGent},
  title       = {GraphQL vs REST: een vergelijkende studie},
  type        = {Bachelorproef},
  url         = {https://catalogus.hogent.be/catalog/hog01:000739595},
  abstract    = {In deze bachelorproef werd er een praktische vergelijking gemaakt tussen REST en GraphQL voor APIs. Dit werd gedaan aan de hand van twee proof-of-concepts, een die gebruik maakte van REST en een van GraphQL.},
  file        = {:pdfs/2857_201862183_PBA-TIN_scriptie.pdf:PDF},
  keywords    = {API, REST, GraphQL},
}

@Article{Informatica2021,
  author       = {Informatica, Inc.},
  date         = {2021-04-01},
  title        = {Accelerator Guide},
  editor       = {Informatica® Data Quality},
  url          = {https://docs.informatica.com/data-quality-and-governance/data-quality/10-5/accelerator-guide/introduction-to-accelerators/accelerators-overview.html},
  urldate      = {2022-10-06},
  version      = {10.5},
  abstract     = {Accelerators are content bundles that address common data quality problems in a country, a region, or an industry. An accelerator might contain mapplets or rule specifications that you can use to analyze and enhance the data in an organization. An accelerator might also contain data domains that you can use to discover the types of information that the data contains.
You import the mapplets, rule specifications, and data domains to the Model repository. Informatica configures the objects to respond to the business rules that you might define for the organization data.},
  file         = {:pdfs/DQ_105_AcceleratorGuide_en.pdf:PDF},
  keywords     = {accelerator, core accelerator, data domains accelerator, financial services accelerator},
  organization = {Informatica Inc.},
  type         = {software},
}

@Article{LopezNovoa2015,
  author       = {Unai Lopez-Novoa and Alexander Mendiburu and Jose Miguel-Alonso},
  date         = {25 February 2014},
  journaltitle = {IEEE Transactions on Parallel and Distributed Systems},
  title        = {A Survey of Performance Modeling and Simulation Techniques for Accelerator-Based Computing},
  doi          = {10.1109/TPDS.2014.2308216},
  issn         = {1558-2183},
  issue        = {1},
  number       = {1},
  pages        = {272--281},
  url          = {https://ieeexplore.ieee.org/abstract/document/6748067},
  urldate      = {2022-10-06},
  volume       = {26},
  abstract     = {The high performance computing landscape is shifting from collections of homogeneous nodes towards heterogeneous systems, in which nodes consist of a combination of traditional out-of-order execution cores and accelerator devices. Accelerators, built around GPUs, many-core chips, FPGAs or DSPs, are used to offload compute-intensive tasks. The advent of this type of systems has brought about a wide and diverse ecosystem of development platforms, optimization tools and performance analysis frameworks. This is a review of the state-of-the-art in performance tools for heterogeneous computing, focusing on the most popular families of accelerators: GPUs and Intel's Xeon Phi. We describe current heterogeneous systems and the development frameworks and tools that can be used for developing for them. The core of this survey is a review of the performance models and tools, including simulators, proposed in the literature for these platforms.},
  file         = {:pdfs/lopez-novoa2015.pdf:PDF},
  journal      = {{IEEE} Transactions on Parallel and Distributed Systems},
  month        = {jan},
  publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
  year         = {2015},
}

@Article{Yin2020,
  author       = {Shihui Yin and Zhewei Jiang and Minkyu Kim and Tushar Gupta and Mingoo Seok and Jae-Sun Seo},
  date         = {14 October 2019},
  journaltitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  title        = {Vesti: Energy-Efficient In-Memory Computing Accelerator for Deep Neural Networks},
  doi          = {10.1109/TVLSI.2019.2940649},
  issn         = {1557-9999},
  number       = {1},
  pages        = {48--61},
  url          = {https://ieeexplore.ieee.org/abstract/document/8867863},
  urldate      = {2022-10-06},
  volume       = {28},
  abstract     = {To enable essential deep learning computation on energy-constrained hardware platforms, including mobile, wearable, and Internet of Things (IoT) devices, a number of digital ASIC designs have presented customized dataflow and enhanced parallelism. However, in conventional digital designs, the biggest bottleneck for energy-efficient deep neural networks (DNNs) has reportedly been the data access and movement. To eliminate the storage access bottleneck, new SRAM macros that support in-memory computing have been recently demonstrated. Several in-SRAM computing works have used the mix of analog and digital circuits to perform XNOR-and-ACcumulate (XAC) operation without row-by-row memory access and can map a subset of DNNs with binary weights and binary activations. In the single array level, large improvement in energy efficiency (e.g., two orders of magnitude improvement) has been reported in computing XAC over digital-only hardware performing the same operation. In this article, by integrating many instances of such in-memory computing SRAM macros with an ensemble of peripheral digital circuits, we architect a new DNN accelerator, titled Vesti. This new accelerator is designed to support configurable multibit activations and large-scale DNNs seamlessly while substantially improving the chip-level energyefficiency with favorable accuracy tradeoff compared to conventional digital ASIC. Vesti also employs double-buffering with two groups of in-memory computing SRAMs, effectively hiding the row-by-row write latencies of in-memory computing SRAMs. The Vesti accelerator is fully designed and laid out in 65-nm CMOS, demonstrating ultralow energy consumption of <; 20 nJ for MNIST classification and <; 40 μJ for CIFAR-10 classification at 1.0-V supply.},
  file         = {:pdfs/yin2019.pdf:PDF},
  journal      = {{IEEE} Transactions on Very Large Scale Integration ({VLSI}) Systems},
  month        = {jan},
  publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
  year         = {2020},
}

@Article{Snehasish2015,
  author       = {Snehasish, Kumar and Naveen, Vedula and Arrvindh, Shriraman and Vijayalakshmi, Srinivasan},
  date         = {08 June 2015},
  journaltitle = {DASX. Proceedings of the 29th ACM on International Conference on Supercomputing 1 },
  title        = {{DASX}},
  doi          = {10.1145/2751205.2751231},
  pages        = {361–372},
  url          = {https://dl.acm.org/doi/abs/10.1145/2751205.2751231},
  abstract     = {Recent research [3,37,38] has proposed compute accelerators to address the energy efficiency challenge. While these compute accelerators specialize and improve the compute efficiency, they have tended to rely on address-based load/store memory interfaces that closely resemble a traditional processor core. The address-based load/store interface is particularly challenging in data-centric applications that tend to access different software data structures. While accelerators optimize the compute section, the address-based interface leads to wasteful instructions and low memory level parallelism (MLP). We study the benefits of raising the abstraction of the memory interface to data structures.

We propose DASX (Data Structure Accelerator), a specialized state machine for data fetch that enables compute accelerators to efficiently access data structure elements in iterative program regions. DASX enables the compute accelerators to employ data structure based memory operations and relieves the compute unit from having to generate addresses for each individual object. DASX exploits knowledge of the program's iteration to i) run ahead of the compute units and gather data objects for the compute unit (i.e., compute unit memory operations do not encounter cache misses) and ii) throttl